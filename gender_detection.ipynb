{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENDER DETECTION PROBLEM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ML ROADMAP**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. D√©finir un objectif mesurable\n",
    "    m√©trique : Pr√©cision, Recall, Score F1\n",
    "2. EDA (Exploratory Data Analysis) : comprendre au maximum les donn√©es dont on dispose\n",
    "    - Analyse de la forme :\n",
    "        - identification de la target\n",
    "        - Nombre de lignes et de colonnes\n",
    "        - Types de variables\n",
    "        - Identification des valeurs manquantes\n",
    "    - Analyze du fond :\n",
    "        - Visualisation de la target (Histogramme / Boxplot)\n",
    "        - Compr√©hension des diff√©rentes variables (Internet)\n",
    "        - Visualisation des relations features-target (Histogramme/ Boxplot)\n",
    "        - Identification des outliers\n",
    "3. Pre-processing\n",
    "    - Cr√©ation du Train set/ Test set\n",
    "    - √âlimination des NaN\n",
    "    - Encodage\n",
    "    - Suppression des outliers n√©faste au mod√®le (Apr√®s le premier mod√®le)\n",
    "    - Feature Selection\n",
    "    - Feature Engineering (Cr√©er de nouvelles variables) (Polynomial feature)(PCA)\n",
    "    - Feature Scaling\n",
    "4. Modeling\n",
    "    - D√©finir une fonction d'√©valuation\n",
    "    - Entrainement de diff√©rents mod√®les\n",
    "    - Optimisation avec GridSearchCV\n",
    "    - (Optionnel) Analyse des erreurs et retour au Preprocessing / EDA\n",
    "    - Learning Curve et prise de d√©cision.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IMPORTING PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (f1_score, \n",
    "                             accuracy_score,\n",
    "                             precision_score,)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model  import SGDClassifier\n",
    "from camel_tools.utils.charmap import CharMapper\n",
    "from sklearn.linear_model import (LogisticRegression, \n",
    "                                  RidgeClassifier)\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     learning_curve, \n",
    "                                     learning_curve, \n",
    "                                     ShuffleSplit, \n",
    "                                     GridSearchCV)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             ConfusionMatrixDisplay)\n",
    "from sklearn.ensemble import (AdaBoostClassifier, \n",
    "                              ExtraTreesClassifier, \n",
    "                              RandomForestClassifier, \n",
    "                              GradientBoostingClassifier, \n",
    "                              HistGradientBoostingClassifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXPLORATORY DATA ANALYSIS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df=pd.read_csv('Datasets/full_names.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's some stats and visualizations üòÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.gender.value_counts(len(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df['name'].apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(names_df.gender.value_counts(len(names_df)), labels=['Women', 'Man'], explode=[0.1, 0], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=600, height=600, max_words=100, background_color='white').generate_from_frequencies(names_df.name.value_counts())\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRE-PROCESSING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defining the pre-processing functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the first place we do some cleaning starting with removing some emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- then we remove all accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def text_to_id(text):\n",
    "    \"\"\"\n",
    "    Convert input text to id.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    if not re.search('[ÿ°-Ÿä]', text):\n",
    "        text = strip_accents(text.lower())\n",
    "        text = re.sub('[^0-9a-zA-Z_-]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we create a function to respelling the translated names according to test that i made mysel.\\\n",
    "For example i found that the \"|\" that was supposed to be an \"A\",  \"p\" thas has to be replaced by an \"a\"\\\n",
    "in the presence of conditions that are defined in the \" arabic translation\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respell(s):\n",
    "    respellings = {\n",
    "                '|': 'A', \n",
    "                \"'\": '', \n",
    "                'p':'a', \n",
    "                \"Y\":'a', \n",
    "                '<':'I', \n",
    "                '$':'sh', \n",
    "                '>': 'A', \n",
    "                '*':'d', \n",
    "                '~':\"\", \n",
    "                '}':\"\", \n",
    "                '&':'a',\n",
    "                'Z':'T'\n",
    "                 }\n",
    "    for wrong in respellings:\n",
    "        try:\n",
    "            index = s.index(wrong)\n",
    "            s = s[:index] + respellings[wrong] + s[len(wrong)+index:]\n",
    "        except:\n",
    "            pass\n",
    "    return ''.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_translation(names_df, ar2bw = CharMapper.builtin_mapper('ar2bw')):\n",
    "    for i, row in names_df.iterrows():\n",
    "        if re.search('[ÿ°-Ÿä]', row['name']):\n",
    "            names_df.at[i, 'name'] = ar2bw(row['name'])\n",
    "            names_df.at[i, 'name'] = respell(row['name']).lower()\n",
    "    return names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encode(names_df, train=True):\n",
    "\n",
    "    # Step 1: Pad names with matrix to make all names same dimension\n",
    "    name_length = 20\n",
    "    nb_char=27\n",
    "    names_df['encoded_name']=[np.zeros((name_length,nb_char)) for name in names_df['name']]\n",
    "\n",
    "    # Step 2: Encode Characters to Numbers\n",
    "    names_df['alpha_name'] = [\n",
    "        [\n",
    "            int(max(0.0, ord(char)-96.0))\n",
    "            for char in name\n",
    "        ]\n",
    "        for name in names_df['name']\n",
    "    ]\n",
    "\n",
    "    # Step 3: Encode names to matrix of 0 and 1\n",
    "    for num in range(len(names_df)):\n",
    "        for i, j in zip(range(len(names_df['alpha_name'][num])), names_df['alpha_name'][num]):\n",
    "            names_df['encoded_name'][num][i,j]=1\n",
    "\n",
    "    return names_df.drop('alpha_name', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this function we gather the previous pre-processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(names_df, train=True):\n",
    "    # Step 1: Lowercase\n",
    "    names_df['name'] = names_df['name'].str.strip()\n",
    "    names_df['name'] = names_df['name'].str.lower()\n",
    "    names_df=names_df.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "    # Step 2: Normalize into english\n",
    "    names_df=arabic_translation(names_df)\n",
    "    names_df['name']= names_df['name'].apply(lambda x: text_to_id(x))\n",
    "    names_df=Encode(names_df)\n",
    "    \n",
    "    X = names_df.iloc[:,-1]\n",
    "    X = np.asarray(X.values.tolist())\n",
    "    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "    if train:\n",
    "        le = LabelEncoder()\n",
    "        # Step 6: Encode Gender to Numbers\n",
    "        names_df['gender'] = le.fit_transform(names_df['gender'])\n",
    "        y = names_df.iloc[:,-2]\n",
    "        # names_df['gender'] = [\n",
    "        #     0.0 if gender=='F' else 1.0 \n",
    "        #     for gender in names_df['gender']\n",
    "        # ]\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.read_csv('Datasets/full_names.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=preprocess(names_df, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODELING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model creation :\n",
    "    - As we know the main objective is to predict the gender which is the target variable 'y', that mean that we're in presence of a suppervised model where \n",
    "        - male : 1\n",
    "        - female : 0\n",
    "    - To make the classification used the following models : \n",
    "        - SVM: SGDClassifier\n",
    "        - ETC: ExtraTreesClassifier\n",
    "        - DTC: DecisionTreeClassifier \n",
    "        - RFC: RandomForestClassifier\n",
    "        - GBC: GradientBoostingClassifier\n",
    "        - HGBC: HistGradientBoostingClassifier\n",
    "        - ADAC: AdaBoostClassifier\n",
    "        - GNB: GaussianNB\n",
    "        - KNN: KNeighborsClassifier\n",
    "        - LR: LogisticRegression\n",
    "        - RC: RidgeClassifier\n",
    "    \n",
    "2. Model evaluation : \n",
    "- We decided to evaluate our model on :\n",
    "    - **Precision** : Precision explains how many of the correctly predicted cases actually turned out to be positive.\\\n",
    "        Precision is useful in the cases where False Positive is a higher concern than False Negatives.\\\n",
    "    - **accuracy** : Recall explains how many of the actual positive cases we were able to predict correctly with our model.\\\n",
    "        It is a useful metric in cases where False Negative is of higher concern than False Positive.\\\n",
    "    - **f1_score** : It gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall.\n",
    "- Which on is most taken in consideration in our case : \n",
    "    - **Precision**\n",
    "- Why ?\n",
    "    - Cause is our case wrong results could lead us to detect false gender of a customer and it can lead it to churn and this could be harmful to the business."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we spllit the data using the cross validation method    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "            #    'SVM' : SGDClassifier(), \n",
    "               'ETC' : ExtraTreesClassifier(),#The Extra-Trees algorithm builds an ensemble of unpruned decision or regression trees according to the classical top-down procedure. \n",
    "                                              #Its two main differences with other tree-based ensemble methods are that it splits nodes by choosing cut-points fully at random and \n",
    "                                              # that it uses the whole learning sample (rather than a bootstrap replica) to grow the trees.\n",
    "            #    'DTC' : DecisionTreeClassifier(), \n",
    "               'RFC' : RandomForestClassifier(n_estimators=200),\n",
    "            #    'GBC' : GradientBoostingClassifier(),\n",
    "            #    'HGBC' : HistGradientBoostingClassifier(),\n",
    "            #    'ADAC' : AdaBoostClassifier(),\n",
    "            #    'GNB' : GaussianNB(),\n",
    "            #    'KNN' : KNeighborsClassifier(),\n",
    "            #    'LR' : LogisticRegression(),\n",
    "            #    'RC' : RidgeClassifier()\n",
    "               }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model training + scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS LEARNINGS AND TESTING\n",
    "f, axes = plt.subplots(1, len(classifiers), figsize=(30, 10), sharey='row')\n",
    "precision = []\n",
    "f1 = []\n",
    "names = []\n",
    "for i, (key, classifier) in enumerate(classifiers.items()):\n",
    "    classifier.fit(X, y)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    precision.append(precision_score(y_test, y_pred)) # accuracy\n",
    "    f1.append(f1_score(y_test, y_pred))\n",
    "    names.append(key)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cf_matrix)\n",
    "    print(key)\n",
    "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
    "    disp.ax_.set_title(key)\n",
    "    disp.im_.colorbar.remove()\n",
    "    disp.ax_.set_xlabel('')\n",
    "    if i!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "\n",
    "f.text(0.4, 0.1, 'Predicted label', ha='left')\n",
    "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "f.colorbar(disp.im_, ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remarked that only 2 models (ETC, RFC) performed well and gived satisfaying results to our problem\n",
    "the first gave 74% of accuracy and the second gave us (75%)\\\n",
    "From there we decided to continue our modeling only with these 2 models and see if there is more improvement that can be made on them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange', 'green']\n",
    "metrics_name = ['Precision', 'F1 score']\n",
    "\n",
    "anot = 0.5 \n",
    "fig, axes = plt.subplots(1, len(classifiers), figsize=(30, 10), sharey='row')\n",
    "for i, metric in enumerate([precision, f1]):\n",
    "    ax = axes[i]\n",
    "    values = ax.barh(np.arange(2), np.array(metric)*100, label= metrics_name[i], color = colors[i])\n",
    "    ax.set_title(metrics_name[i], fontsize = 20)\n",
    "    for s in ['top', 'right']:\n",
    "        ax.spines[s].set_visible(False)\n",
    " \n",
    "    # Remove x, y Ticks\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    \n",
    "    # Add padding between axes and labels\n",
    "    ax.xaxis.set_tick_params(pad = 5)\n",
    "    ax.yaxis.set_tick_params(pad = 10)\n",
    "    \n",
    "    # Add x, y gridlines\n",
    "    ax.grid(visible = True, color ='grey',\n",
    "            linestyle ='-.', linewidth = 0.5,\n",
    "            alpha = 0.2)\n",
    "    \n",
    "    # Show top values\n",
    "    ax.invert_yaxis()\n",
    "    # Add annotation to bars\n",
    "    ax.bar_label(values, fontsize=12)\n",
    "    \n",
    "    anot = 1\n",
    "# Add Text watermark\n",
    "fig.text(0.9, 1, 'amine Zidelmal', fontsize = 15,\n",
    "    color ='black', ha ='right', va ='top',\n",
    "    alpha = 0.7)\n",
    "# f.text(0.4, 0.1, 'Objectives visualisations', ha='center')\n",
    "\n",
    "fig.text(0.09, 0.25, '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'.join(names), fontsize = 20, rotation=0, ha='left')\n",
    "fig.text(0.55, 0.25, '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'.join(names), fontsize = 20, rotation=0, ha='left')\n",
    "fig.text(0.5, 0.07, 'Metric rate (%)', fontsize = 20, ha='left')\n",
    "fig.text(0.07, 0.5, 'Models', fontsize = 20, ha='left')\n",
    "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [8],\n",
    "}\n",
    "\n",
    "gs_rfc=GridSearchCV(RandomForestClassifier(), param_grid, cv=5, verbose=1, n_jobs=1)\n",
    "gs_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After using GridSearchCV we found that the best params for RFC were : n_estimators = 200 and max_depth = 8\n",
    "- For the ETC the predifined params were already the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Error analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remarked that there is some errors when marking the model learn on all the data set and testing on the X_test,\\\n",
    "so we had to see where is the problem.\\\n",
    "And in fact we found that there is some names that were not well labeled and we fixed this mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df['name'] = names_df['name'].str.strip()\n",
    "names_df['name'] = names_df['name'].str.lower()\n",
    "names_df=names_df.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "X_xtrain, X_xtest, y_xtrain, y_xtest = train_test_split(names_df.iloc[:,0], names_df.iloc[:,-1], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=pd.DataFrame({'NAME':X_xtest, \"Gender\":y_xtest, 'pred':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['Gender'] = LabelEncoder().fit_transform(testdata['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata[testdata.Gender!=testdata.pred]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, train_score, val_score = learning_curve(classifier, X_train, y_train, train_sizes=np.linspace(0.2, 1.0, 10), cv=5)\n",
    "print(N)\n",
    "plt.plot(N,train_score.mean(axis=1), label='train')\n",
    "plt.plot(N,val_score.mean(axis=1), label='validation')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning curves show us how the performance of a classifier changes.**\\\n",
    "So, on this curve you can see both the training and the cross-validation score. The training score doesn‚Äôt change much by adding more examples.\\\n",
    "But the cross-validation score definitely does! We can see that the performances didn't reach it max potentiel at 87%.\n",
    "\n",
    "So, what this tells us is that adding more examples over the ones we currently have is probably is required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pickle the best model üòÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifiers['RFC'], 'RFC.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
